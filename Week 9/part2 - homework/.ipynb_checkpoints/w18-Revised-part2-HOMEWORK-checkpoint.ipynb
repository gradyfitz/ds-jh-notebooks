{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop 18 Revised Part 2\n",
    "Since there's a lot to cover, we'll focus on doing things \"by hand\" to cement your understanding of the concepts. As homework, you are expected to go through the code notebooks as you will need to know how to do it for the group project!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <u> Class Discussion Concept: Pearson Correlation Coefficient </u>\n",
    "The Pearson Correlation Coefficient measures the **linear relationship between two data variables**. \n",
    "- The value of Pearson Correlation Coefficient varies between `-1` and `+1`, with 0 implying no correlation. \n",
    "    - Correlations of `-1` or `+1` imply an exact linear relationship. \n",
    "    - Positive correlations imply that as x increases, so does y. \n",
    "    - Negative correlations imply that as x increases, y decreases.\n",
    "- It's good to know that this correlation coefficient is *not robust* as it can be affected by outliers.\n",
    "\n",
    "Requirements/Assumptions for using the Pearson's correlation coefficient:\n",
    "- Scale of measurement should be an interval or ratio.\n",
    "- Variables should be approximately normally distributed if a significance test is to be trusted.\n",
    "- The association should be linear (can be confirmed using a visualisation).\n",
    "\n",
    "## Formula and Derivation\n",
    "Given paired data ${\\displaystyle \\left\\{(x_{1},y_{1}),\\ldots ,(x_{n},y_{n})\\right\\}}{\\displaystyle \\left\\{(x_{1},y_{1}),\\ldots ,(x_{n},y_{n})\\right\\}} $ consisting of ${\\displaystyle n} $  pairs, perason correlation coffeceint ${\\displaystyle r_{xy}}$ between them is defined as:\n",
    "\n",
    "$${\\displaystyle r_{xy}={\\frac {\\sum _{i=1}^{n}(x_{i}-{\\bar {x}})(y_{i}-{\\bar {y}})}{{\\sqrt {\\sum _{i=1}^{n}(x_{i}-{\\bar {x}})^{2}}}{\\sqrt {\\sum _{i=1}^{n}(y_{i}-{\\bar {y}})^{2}}}}}}  (Eq. 1) $$ \n",
    "where: \n",
    "\n",
    "- ${\\displaystyle n}$ is sample size \n",
    "- ${\\displaystyle x_{i},y_{i}}$ are the individual sample points indexed with $i$\n",
    "- ${\\textstyle {\\bar {x}}={\\frac {1}{n}}\\sum _{i=1}^{n}x_{i}} $ is the  sample mean \n",
    "\n",
    "Whilst it's good to have the formula in front of you, it is quite important and useful to know how this is calculated (refer to the above requirements/assumptions).\n",
    "\n",
    "As a class, go through the derivation of the Pearson Correlation Coefficient and discuss the following:\n",
    "1. Mean\n",
    "2. Variance\n",
    "3. Standard Deviation\n",
    "4. Covariance\n",
    "5. Pearson Correlation Coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Discussion Correlation Exercise 1 (By Hand)\n",
    "Consider the following hypothetical dataset providing measurements for `Average Steps per day` and `Average Resting Heart Rate`, across a sample of $n=12$ people.\n",
    "    \n",
    "Person ID | Average steps per day | Average resting hear rate \n",
    "--- | --- | --- \n",
    "1  | 1000 | 100 \n",
    "2  | 2500 | 105 \n",
    "3  | 3000 | 80\n",
    "4  | 5000 | 77 \n",
    "5  | 6000 | 74 \n",
    "6  | 9000 | 70 \n",
    "7  | 11000 | 65 \n",
    "8  | 14000 | 63 \n",
    "9  | 18000 | 62 \n",
    "10 | 19000 | 61 \n",
    "11 | 19500 | 60.5   \n",
    "12 | 22000 | 55     \n",
    "    \n",
    "    \n",
    "Compute the Pearson correlation between `Average Steps per day` and `Average Resting Heart Rate`. Show your working. How would you interpret this correlation value?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Discussion Exercise 1\n",
    "- Based on the Pearson correlation value, can one conclude that doing more steps per day will cause one's average resting heart rate to decrease? \n",
    "- How else might it be interpreted?\n",
    "\n",
    "Then, as a class, discuss appropriate ways of reaching _conclusions_ with the Pearson Correlation Coefficient. Remember, the correlation coefficient is **one of many metrics** and should never be the sole metric used to reach a statistical conclusion!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Correlation Homework: Practice using Python \n",
    "## About the dataset\n",
    "Each row in this dataset captures a customer booking:\n",
    "- `unit_price`: is the price per ticket\n",
    "- `quantity`: is the number of tickets in that booking\n",
    "- `time_on_site`: is the amount of seconds it takes between the customer first logged onto the site and when they finished placing the booking\n",
    "- `movie_rating`: is the average customer rating of that movie (at the time of the booking)\n",
    "- `label`: is the company's internal classification of that booking. \n",
    "    - `Low` if they booked <= 5 tickets; \n",
    "    - `Medium` if they booked between 6 and 10 tickets; \n",
    "    - and `High` if the purchase was more than 10 tickets.\n",
    "\n",
    "## Learning objectives\n",
    "- Calculate Pearson-$r$ correlation by hand.\n",
    "- Apply Pearson-$r$ correlation using the `pandas` library and plotting it using `matplotlib.pyplot`.\n",
    "- Learn to discretize data manually using domain knowledge. \n",
    "- Then apply discretization on data using some `sklearn` library functions. \n",
    "- Know how to calculate Mutual Information by hand. \n",
    "- Finally, apply Mutual Information using `scipy` library functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T02:26:50.183484Z",
     "start_time": "2021-12-06T02:26:49.578632Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T02:26:50.201484Z",
     "start_time": "2021-12-06T02:26:50.184691Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('correlation.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "First we will draw a scatter plot to observe the  linear relationship between `movie rating` and `time on site`. We do so as we must assume a linear relationship exists in order to compute correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T02:26:50.289926Z",
     "start_time": "2021-12-06T02:26:50.202425Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(x=data['movie_rating'], y=data['time_on_site'], alpha=0.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Here, we can observe that there is a negative linear relationship between `movie_rating` and `time_on_site` using a scatter plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We can also simply compute the correlation using a `pandas.DataFrame()` method!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T02:26:50.297482Z",
     "start_time": "2021-12-06T02:26:50.291472Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "FEATURES = ['movie_rating', 'time_on_site']\n",
    "\n",
    "data[FEATURES].corr(method='pearson')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Question: As a class, discuss the output of the correlation matrix above. Additionally, what does it mean to say _\"correlation does not imply causality?\"_**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's now look at **Correlation Heatmaps** which is essentially a heatmap equivalent of the matrix above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T02:26:50.402455Z",
     "start_time": "2021-12-06T02:26:50.298348Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sns.heatmap(data[FEATURES].corr(), # get the Pearson correlation\n",
    "            annot=True # annotate the squares with the values\n",
    ")\n",
    "\n",
    "plt.yticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- We can see that darker colours indicate negative correlation, and lighter colours with positive correlation.\n",
    "- Be careful with *wording* when it comes to correlation. **Correlation does not imply causality**.\n",
    "\n",
    "For example, _\"we can see that there seems to be a strong negative relationship between the maximum capacity and the number of tickets sold\"_. In other words, the larger the maximum capacity, the fewer number of tickets have been sold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## IMPORTANT: Inappropriate Use Cases of Pearson Correlation ###\n",
    "- Correlation is best suited to *continuous*, *normally distributed* data and is thus easily swayed by extreme values. As such, correlation will misrepresent relationships that are non-linear and/or non-continuous. \n",
    "\n",
    "In the following example, the relationship between `time_on_site` and `quantity` is non-linear. However, the calculated correlation value misrepresents the relationship between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T02:26:50.477455Z",
     "start_time": "2021-12-06T02:26:50.403667Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(data['time_on_site'], data['quantity'], alpha=0.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T02:26:50.484940Z",
     "start_time": "2021-12-06T02:26:50.478560Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "INCORRECT_FEATURES = ['time_on_site', 'quantity']\n",
    "\n",
    "data[INCORRECT_FEATURES].corr(method='pearson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T02:26:50.573923Z",
     "start_time": "2021-12-06T02:26:50.486150Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data.hist('time_on_site')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T02:26:50.689471Z",
     "start_time": "2021-12-06T02:26:50.574880Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data.hist('quantity')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <u>Class Discussion Concept: Discretization</u>\n",
    "Discretization is the process through which we can transform continuous variables into a discrete form. \n",
    "\n",
    "- We do this by creating a set of intervals (better known as **bins**) that go across the range or frequency of our desired variable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Discussion Discretization Exercise 1 (By Hand)\n",
    "Apply `3 bin equal frequency` discretisation to `Average Steps per day` and `4 bin equal frequency` discretisation to `Average Resting Heart Rate`. Show the values of the discretised features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Discretization Homework: Practice using Python \n",
    "## Discretization via Domain Knowledge\n",
    "- Domain Knowledge is known as the \"business context\" or \"background context\" behind the dataset.\n",
    "- Whilst numbers can tell you a lot of information, it is always good to refer back to the actual meaning behind the dataset.\n",
    "\n",
    "First we will discretize the continuous variable `quantity` into 3 bins based on our \"domain knowledge\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T02:26:50.697426Z",
     "start_time": "2021-12-06T02:26:50.691571Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def bin_quantity(x):\n",
    "    \"\"\"\n",
    "    Domain knowledge oriented discretization with the following bins:\n",
    "    \n",
    "        [min, 5), [5, 10), [10, max] \n",
    "        \n",
    "    Remeber that [] denote inclusive, () denote exclusive.\n",
    "    \"\"\"\n",
    "    LOWER = 5\n",
    "    UPPER = 10\n",
    "    \n",
    "    # [min, 5)\n",
    "    if x < LOWER:\n",
    "        return 0\n",
    "    # [5, 10)\n",
    "    elif LOWER <= x < UPPER:\n",
    "        return 1 \n",
    "    # [10, max] \n",
    "    return 2\n",
    "\n",
    "# apply the function on the series\n",
    "data['binned_quantity'] = data['quantity'].apply(bin_quantity)\n",
    "\n",
    "# also, print out the min/max for this feature\n",
    "min(data['quantity']), max(data['quantity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T02:26:50.793440Z",
     "start_time": "2021-12-06T02:26:50.698503Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data['quantity'].hist()\n",
    "\n",
    "plt.title('Distribution of quantity before binning')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T02:26:50.883960Z",
     "start_time": "2021-12-06T02:26:50.794614Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data['binned_quantity'].hist()\n",
    "\n",
    "plt.title('Distribution of quantity after binning')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "You'll notice above that the bins are now looking a bit sparse. That's expected since we discretized our values into 0, 1, and 2.\n",
    "\n",
    "In order to make it look a \"bit nicer\", we can specify `bins=3` to denote the width."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T02:26:50.966456Z",
     "start_time": "2021-12-06T02:26:50.885222Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data.hist('binned_quantity', bins=3)\n",
    "\n",
    "plt.title('Distribution quantity after binning (with bins=3)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Homework Exercise 1\n",
    "Play around with the `bins=` argument. See if you can recreate the same distribution with the `bin_quantity` with bin sizes `[min, 5), [5, 10), [10, max]`. \n",
    "\n",
    "Hint: You should refer to the min/max output above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T02:26:51.036955Z",
     "start_time": "2021-12-06T02:26:50.967605Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.hist(data['quantity'], \n",
    "         bins=[1, 5, 10, 12] # ANSWER\n",
    ")\n",
    "\n",
    "plt.title('Distribution of quantity after binning')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Lastly, we'll use `sklearn`'s version of discretization to bin `time_on_site`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T02:26:51.074972Z",
     "start_time": "2021-12-06T02:26:51.037918Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "equal_width = KBinsDiscretizer(n_bins=3, # number of bins\n",
    "                               # the method and strategy which are out-of-scope for this subject, but here's the explanation\n",
    "                               # in case you want it anyways.\n",
    "                               \n",
    "                               # ordinal refers to categorical data that has some ordering i.e cold, warm, hot\n",
    "                               encode='ordinal', \n",
    "                               # strategy is used to define how wide the bins are. Uniform = equal size.\n",
    "                               strategy='uniform')\n",
    "\n",
    "# a transformed version of \n",
    "data['binned_time_on_site'] = equal_width.fit_transform(data['time_on_site'].to_numpy(). \\\n",
    "                                                        reshape(-1, 1)).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <u> Class Discussion Concept: Entropy <u>\n",
    "Entropy is the amount of _information_ or _uncertainty_ in some variables' possible outcome. For example:\n",
    "    - An unbiased coin flip (50% Heads, 50% Tails) will have an entropy of 1. \n",
    "    - A biased coin flip (100% Heads, 0% Tails) will have an entropy of 0. \n",
    "    \n",
    "From above, hopefully you can see that the entropy is 1 with the unbiased coin flip because there is one _uncertainty_ in the flip (either heads or tails). Likewise, in the biased coin flip, we have no _uncertainty_ (100% certain we are getting Heads), hence, the entropy is 0.\n",
    "    \n",
    "It originates from Shannon's Entropy (which is to do with bits in networking), hence, the reason why you should be using the logarithm base 2 when doing calculations.\n",
    "    \n",
    "The entropy is given to you as:\n",
    "    \n",
    "$$\n",
    "H(X) = -\\sum P(X=x_i)\\log_2(P(X=x_i)) \n",
    "$$\n",
    "    \n",
    "1. $P(X=x_i)$ represents the event $x_i$ occurring.\n",
    "2. For binning, $P(X=x_i)$ specifically represents the probability of a data value belonging to a specific bin.\n",
    "    \n",
    "## <u> Concept: Conditional Entropy <u>\n",
    "The conditional entropy is the amount of _information_ to describe an outcome of $Y$ given that $X$ occurred.\n",
    "    - It's good to know that if $Y$ is completely determined by $X$, then $H(Y|X)=0$.\n",
    "    \n",
    "    \n",
    "The conditional entropy is given to you as:\n",
    "    \n",
    "$$\n",
    "H(Y|X) = \\sum P(X=x_i)\\log_2(P(Y | X=x_i)) \n",
    "$$\n",
    "    \n",
    "That is, the probability of $x_i$ occurring _times_ the entropy of $Y$ given that $x_i$ occurred. \n",
    "    - You can think of it as \"locking\" or \"fixing\" the condition on event $x_i$, then compute $Y$ with the remaining values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <u> Class Discussion Concept: Mutual Information (MI) <u>\n",
    "Mutual information is calculated between two variables and measures the reduction in uncertainty for one variable given a known value of the other variable. It is considered more general than correlation and handles non-linear dependencies and discrete random variables.\n",
    "\n",
    "In other words:\n",
    "    - It is the amount of _dependence_ between two random variables.\n",
    "    - The larger the MI, the more dependent or correlated they are.\n",
    "    - In contrast, the greater the _entropy_, the more _uncertain_ we are. \n",
    "\n",
    "\n",
    "The mutual information between two random variables $X$ and $Y$ can be stated formally as follows:\n",
    "\n",
    "$$\n",
    "\\text{MI}(X, Y) = H(X) â€“ H(X | Y) = H(Y) - H(Y | X)\n",
    "$$\n",
    "Where,\n",
    "- $\\text{MI}(X, Y)$ is the mutual information for $X$ and $Y$\n",
    "- $H(X)$ is the entropy for $X$ \n",
    "- $H(X|Y)$ is the conditional entropy for $X$ given $Y$. \n",
    "\n",
    "#### Advanced\n",
    "_Normalized Mutual Information (NMI)_ is a normalization of MI to scale the results between 0 (no mutual information) and 1 (perfect correlation). It is computed using the following formula:\n",
    "\n",
    "$$\n",
    "\\text{NMI}(X, Y) = \\frac{\\text{MI}(X,Y)}{min(H(X),H(Y))}\n",
    "$$\n",
    "Where,\n",
    "- $\\text{MI}(X,Y)$ is the mutual information for $X$ and $Y$\n",
    "- $H(X)$ is the entropy for $X$ \n",
    "- $H(Y)$ is the entropy for $Y$ \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Class Discussion Entropy Exercises (By Hand)\n",
    "Here's the same dataset we worked with for correlation.\n",
    "    \n",
    "Person ID | Average steps per day | Average resting hear rate \n",
    "--- | --- | --- \n",
    "1  | 1000 | 100 \n",
    "2  | 2500 | 105 \n",
    "3  | 3000 | 80\n",
    "4  | 5000 | 77 \n",
    "5  | 6000 | 74 \n",
    "6  | 9000 | 70 \n",
    "7  | 11000 | 65 \n",
    "8  | 14000 | 63 \n",
    "9  | 18000 | 62 \n",
    "10 | 19000 | 61 \n",
    "11 | 19500 | 60.5   \n",
    "12 | 22000 | 55     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy Exercise 1 (By Hand)\n",
    "Using the discretized features, compute the following entropies: \n",
    "- `H(Average Steps per day)` \n",
    "- `H(Average Resting Heart Rate)`\n",
    "- `H(Average steps per day | Average Resting Heart Rate)`\n",
    "- `H(Average Resting Heart Rate | Average Steps per day)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy Exercise 2 (By Hand)\n",
    "Using the above information, compute the mutual information between:  \n",
    "- `Average Steps per day`\n",
    "- `Average Resting Heart Rate`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Entropy Homework: Practice using Python\n",
    "Now, lets practice to compute MI using python. First we will write a function to compute the NMI between `quantity` and  `time_on_site`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T02:26:51.078955Z",
     "start_time": "2021-12-06T02:26:51.076117Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def compute_probability(col):\n",
    "    \"\"\"\n",
    "    Compute the probability of a certain event\n",
    "    \"\"\"\n",
    "    return col.value_counts() / col.shape[0]\n",
    "\n",
    "def compute_entropy(col):\n",
    "    \"\"\"\n",
    "    Compute the entropy of a certain event\n",
    "    \"\"\"\n",
    "    probabilities = compute_probability(col)\n",
    "    entropy = -sum(probabilities * np.log2(probabilities))\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T02:26:51.084956Z",
     "start_time": "2021-12-06T02:26:51.080293Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# for the binned quantity\n",
    "print(\"Probabilities\")\n",
    "print(compute_probability(data['binned_quantity']))\n",
    "print(\"\\nEntropy for binned_quantity\")\n",
    "print(compute_entropy(data['binned_quantity']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T02:26:55.062664Z",
     "start_time": "2021-12-06T02:26:55.058739Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# for binned time on site\n",
    "print(compute_entropy(data['binned_time_on_site']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T02:26:55.521182Z",
     "start_time": "2021-12-06T02:26:55.510951Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def compute_conditional_entropy(x, y):\n",
    "    \"\"\"\n",
    "    Compute the conditional entropy between two random variables.\n",
    "    Specifically, the conditional entropy of Y given X.\n",
    "    \"\"\"\n",
    "    probability_x = compute_probability(x)\n",
    "    \n",
    "    temp_df = pd.DataFrame({'X': x, 'Y': y})\n",
    "    \n",
    "    conditional_entropy = 0\n",
    "    \n",
    "    # for unique event x_i\n",
    "    for x_i in x.unique():\n",
    "        # get the data for Y given X=x_i\n",
    "        y_given_x = temp_df.loc[temp_df['X'] == x_i, 'Y']\n",
    "        \n",
    "        # compute the conditional entropy\n",
    "        conditional_entropy += probability_x[x_i] * compute_entropy(y_given_x)\n",
    "    \n",
    "    return conditional_entropy\n",
    "\n",
    "print(compute_conditional_entropy(data['binned_time_on_site'], data['binned_quantity']))\n",
    "print(compute_conditional_entropy(data['binned_quantity'], data['binned_time_on_site']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T02:26:55.942657Z",
     "start_time": "2021-12-06T02:26:55.935267Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def NMI(x, y):\n",
    "    \"\"\"\n",
    "    Compute the NMI between two random variables\n",
    "    \"\"\"\n",
    "    # H(X) and H(Y)\n",
    "    entropy_x = compute_entropy(x)\n",
    "    entropy_y = compute_entropy(y)\n",
    "    \n",
    "    # H(Y|X)\n",
    "    conditional_entropy = compute_conditional_entropy(x, y)\n",
    "    \n",
    "    # MI(X, Y)\n",
    "    MI = entropy_y - conditional_entropy \n",
    "    # same as MI = entropy_x - compute_conditional_entropy(y, x)\n",
    "    \n",
    "    return MI / min(entropy_x, entropy_y)\n",
    "\n",
    "print(NMI(data['binned_time_on_site'], data['binned_quantity']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We can also simply compute the NMI using `sklearn` [normalized_mutual_info_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.normalized_mutual_info_score.html).\n",
    "\n",
    "Now, we will use this function to find the NMI between `time_on_site` and `quantity`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T02:26:58.533814Z",
     "start_time": "2021-12-06T02:26:58.518692Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import normalized_mutual_info_score\n",
    "\n",
    "normalized_mutual_info_score(data['binned_time_on_site'], \n",
    "                             data['binned_quantity'], \n",
    "                             \n",
    "                             # we use min here because in NMI, we want to have our demoninator\n",
    "                             # to be min(H(X), H(Y))\n",
    "                             average_method='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a56ef4f8f84c44b3b227f685597acccb1d96f2f638ff5d164cf8fb466a77a95f"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
